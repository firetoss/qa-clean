"""
QA Clean å‘½ä»¤è¡Œæ¥å£
"""

import argparse
import sys
import os
from pathlib import Path
from typing import List, Dict, Any, Optional
import pandas as pd
from .processor import QAProcessor
from .config import get_recommended_store_type, get_store_config, list_available_stores


def main():
    """ä¸»å‡½æ•°"""
    parser = create_argument_parser()
    args = parser.parse_args()
    
    try:
        if args.command == "process":
            process_qa_data(args)
        elif args.command == "search":
            search_similar_questions(args)
        elif args.command == "info":
            show_vector_store_info(args)
        else:
            parser.print_help()
            
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        sys.exit(1)


def create_argument_parser() -> argparse.ArgumentParser:
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description="QA æ•°æ®æ¸…æ´—ä¸æ²»ç†å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å¤„ç†QAæ•°æ®ï¼ˆä½¿ç”¨FAISS GPUï¼Œé»˜è®¤ï¼‰
  qa-clean process data.csv --output results.csv
  
  # ä½¿ç”¨PostgreSQL+pgvectorå­˜å‚¨
  qa-clean process data.csv --vector-store pgvector --output results.csv
  
  # æœç´¢ç›¸ä¼¼é—®é¢˜
  qa-clean search "å¦‚ä½•å®‰è£…Python?" --vector-store faiss_gpu
  
  # æŸ¥çœ‹å­˜å‚¨ä¿¡æ¯
  qa-clean info --vector-store faiss_gpu
        """
    )
    
    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")
    
    # process å‘½ä»¤
    process_parser = subparsers.add_parser("process", help="å¤„ç†QAæ•°æ®")
    process_parser.add_argument("input", help="è¾“å…¥æ–‡ä»¶è·¯å¾„ (CSV/Excel)")
    process_parser.add_argument("--output", "-o", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    process_parser.add_argument("--vector-store", choices=["faiss_gpu", "pgvector"], 
                              default="faiss_gpu", help="å‘é‡å­˜å‚¨ç±»å‹ (é»˜è®¤: faiss_gpu)")
    process_parser.add_argument("--topk", type=int, default=100, help="ç›¸ä¼¼åº¦æœç´¢top-kå€¼ (é»˜è®¤: 100)")
    process_parser.add_argument("--question-col", default="question", help="é—®é¢˜åˆ—å (é»˜è®¤: question)")
    process_parser.add_argument("--answer-col", default="answer", help="ç­”æ¡ˆåˆ—å (é»˜è®¤: answer)")
    
    # FAISS GPU ç‰¹å®šå‚æ•°
    process_parser.add_argument("--gpu-id", type=int, default=0, help="GPUè®¾å¤‡ID (é»˜è®¤: 0)")
    
    # PostgreSQL ç‰¹å®šå‚æ•°
    process_parser.add_argument("--pg-host", help="PostgreSQLä¸»æœºåœ°å€")
    process_parser.add_argument("--pg-port", type=int, help="PostgreSQLç«¯å£")
    process_parser.add_argument("--pg-db", help="PostgreSQLæ•°æ®åº“å")
    process_parser.add_argument("--pg-user", help="PostgreSQLç”¨æˆ·å")
    process_parser.add_argument("--pg-password", help="PostgreSQLå¯†ç ")
    
    # search å‘½ä»¤
    search_parser = subparsers.add_parser("search", help="æœç´¢ç›¸ä¼¼é—®é¢˜")
    search_parser.add_argument("query", help="æŸ¥è¯¢é—®é¢˜")
    search_parser.add_argument("--vector-store", choices=["faiss_gpu", "pgvector"], 
                              default="faiss_gpu", help="å‘é‡å­˜å‚¨ç±»å‹ (é»˜è®¤: faiss_gpu)")
    search_parser.add_argument("--topk", type=int, default=10, help="è¿”å›ç»“æœæ•°é‡ (é»˜è®¤: 10)")
    search_parser.add_argument("--gpu-id", type=int, default=0, help="GPUè®¾å¤‡ID (é»˜è®¤: 0)")
    
    # info å‘½ä»¤
    info_parser = subparsers.add_parser("info", help="æ˜¾ç¤ºå‘é‡å­˜å‚¨ä¿¡æ¯")
    info_parser.add_argument("--vector-store", choices=["faiss_gpu", "pgvector"], 
                           help="æŒ‡å®šå­˜å‚¨ç±»å‹ï¼Œä¸æŒ‡å®šåˆ™æ˜¾ç¤ºæ‰€æœ‰")
    
    return parser


def process_qa_data(args):
    """å¤„ç†QAæ•°æ®"""
    print(f"ğŸ”„ å¼€å§‹å¤„ç†QAæ•°æ®: {args.input}")
    
    # è¯»å–æ•°æ®
    data = read_input_file(args.input, args.question_col, args.answer_col)
    if not data:
        print("âŒ æ²¡æœ‰è¯»å–åˆ°æœ‰æ•ˆæ•°æ®")
        return
    
    print(f"ğŸ“Š è¯»å–åˆ° {len(data)} æ¡QAæ•°æ®")
    
    # å‡†å¤‡å‘é‡å­˜å‚¨å‚æ•°
    store_kwargs = prepare_store_kwargs(args)
    
    # åˆ›å»ºå¤„ç†å™¨
    with QAProcessor(topk=args.topk, vector_store_type=args.vector_store, **store_kwargs) as processor:
        # å¤„ç†æ•°æ®
        results = processor.process_qa_data(data)
        
        # è¾“å‡ºç»“æœ
        print(f"\nâœ… å¤„ç†å®Œæˆ!")
        print(f"   åŸå§‹æ•°æ®: {results['total_count']} æ¡")
        print(f"   å»é‡å: {results['unique_count']} æ¡")
        print(f"   èšç±»æ•°é‡: {len(results['representative_questions'])} ä¸ª")
        
        # ä¿å­˜ç»“æœ
        if args.output:
            save_results(results, args.output)
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {args.output}")


def search_similar_questions(args):
    """æœç´¢ç›¸ä¼¼é—®é¢˜"""
    print(f"ğŸ” æœç´¢ç›¸ä¼¼é—®é¢˜: {args.query}")
    
    # å‡†å¤‡å‘é‡å­˜å‚¨å‚æ•°
    store_kwargs = prepare_store_kwargs(args)
    
    # åˆ›å»ºå¤„ç†å™¨
    with QAProcessor(topk=args.topk, vector_store_type=args.vector_store, **store_kwargs) as processor:
        # æœç´¢ç›¸ä¼¼é—®é¢˜
        results = processor.search_similar_questions(args.query, args.topk)
        
        if not results:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼é—®é¢˜")
            return
        
        print(f"\nâœ… æ‰¾åˆ° {len(results)} ä¸ªç›¸ä¼¼é—®é¢˜:")
        for i, result in enumerate(results, 1):
            print(f"\n{i}. ç›¸ä¼¼åº¦: {result['similarity']:.3f}")
            print(f"   é—®é¢˜: {result['text_content']}")
            if 'metadata' in result and result['metadata']:
                print(f"   å…ƒæ•°æ®: {result['metadata']}")


def show_vector_store_info(args):
    """æ˜¾ç¤ºå‘é‡å­˜å‚¨ä¿¡æ¯"""
    if args.vector_store:
        # æ˜¾ç¤ºç‰¹å®šå­˜å‚¨ç±»å‹ä¿¡æ¯
        config = get_store_config(args.vector_store)
        print(f"ğŸ“Š {config['name']} ({args.vector_store})")
        print(f"æè¿°: {config['description']}")
        print(f"ä¼˜åŠ¿: {', '.join(config['pros'])}")
        print(f"åŠ£åŠ¿: {', '.join(config['cons'])}")
        print(f"é€‚ç”¨åœºæ™¯: {', '.join(config['recommended_for'])}")
    else:
        # æ˜¾ç¤ºæ‰€æœ‰å­˜å‚¨ç±»å‹ä¿¡æ¯
        print("ğŸ” å¯ç”¨çš„å‘é‡å­˜å‚¨ç±»å‹:\n")
        for store_type in list_available_stores():
            config = get_store_config(store_type)
            print(f"ğŸ“Š {config['name']} ({store_type})")
            print(f"   æè¿°: {config['description']}")
            print(f"   ä¼˜åŠ¿: {', '.join(config['pros'])}")
            print(f"   åŠ£åŠ¿: {', '.join(config['cons'])}")
            print(f"   é€‚ç”¨åœºæ™¯: {', '.join(config['recommended_for'])}")
            print()
        
        # æ˜¾ç¤ºæ¨è
        recommended = get_recommended_store_type()
        print(f"ğŸ’¡ å½“å‰ç¯å¢ƒæ¨è: {recommended}")


def read_input_file(file_path: str, question_col: str, answer_col: str) -> List[Dict[str, Any]]:
    """è¯»å–è¾“å…¥æ–‡ä»¶"""
    file_path = Path(file_path)
    
    if not file_path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©è¯»å–æ–¹æ³•
    if file_path.suffix.lower() == '.csv':
        df = pd.read_csv(file_path)
    elif file_path.suffix.lower() in ['.xlsx', '.xls']:
        df = pd.read_excel(file_path)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
    
    # æ£€æŸ¥å¿…è¦çš„åˆ—
    if question_col not in df.columns:
        raise ValueError(f"é—®é¢˜åˆ— '{question_col}' ä¸å­˜åœ¨")
    if answer_col not in df.columns:
        raise ValueError(f"ç­”æ¡ˆåˆ— '{answer_col}' ä¸å­˜åœ¨")
    
    # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
    data = []
    for _, row in df.iterrows():
        data.append({
            'question': str(row[question_col]),
            'answer': str(row[answer_col]),
            'metadata': {col: str(val) for col, val in row.items() 
                        if col not in [question_col, answer_col] and pd.notna(val)}
        })
    
    return data


def prepare_store_kwargs(args) -> Dict[str, Any]:
    """å‡†å¤‡å‘é‡å­˜å‚¨å‚æ•°"""
    kwargs = {}
    
    if args.vector_store == "faiss_gpu":
        kwargs["gpu_id"] = args.gpu_id
    
    elif args.vector_store == "pgvector":
        # ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–å‘½ä»¤è¡Œå‚æ•°
        kwargs["connection_params"] = {
            "host": args.pg_host or os.getenv("POSTGRES_HOST", "localhost"),
            "port": args.pg_port or int(os.getenv("POSTGRES_PORT", "5432")),
            "database": args.pg_db or os.getenv("POSTGRES_DB", "qa_clean"),
            "user": args.pg_user or os.getenv("POSTGRES_USER", "postgres"),
            "password": args.pg_password or os.getenv("POSTGRES_PASSWORD", "password"),
        }
    
    return kwargs


def save_results(results: Dict[str, Any], output_path: str):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    output_path = Path(output_path)
    
    # å‡†å¤‡è¾“å‡ºæ•°æ®
    output_data = []
    for i, qa_data in enumerate(results['dedup_results']):
        cluster_id = None
        for cluster_idx, cluster_indices in enumerate(results['clusters']):
            if i in cluster_indices:
                cluster_id = cluster_idx
                break
        
        output_data.append({
            'id': i,
            'question': qa_data['question'],
            'answer': qa_data['answer'],
            'cluster_id': cluster_id,
            'representative_question': results['representative_questions'].get(cluster_id, ''),
            'metadata': qa_data.get('metadata', {})
        })
    
    # åˆ›å»ºDataFrameå¹¶ä¿å­˜
    df = pd.DataFrame(output_data)
    
    if output_path.suffix.lower() == '.csv':
        df.to_csv(output_path, index=False, encoding='utf-8')
    elif output_path.suffix.lower() in ['.xlsx', '.xls']:
        df.to_excel(output_path, index=False)
    else:
        # é»˜è®¤ä¿å­˜ä¸ºCSV
        output_path = output_path.with_suffix('.csv')
        df.to_csv(output_path, index=False, encoding='utf-8')


if __name__ == "__main__":
    main()


