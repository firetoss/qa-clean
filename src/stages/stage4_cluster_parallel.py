"""
Stage4 èšç±»æ¨¡å— - å¹¶è¡Œå¼•æ“å®ç°

å¤šæ ¸å¹¶è¡Œä¼˜åŒ–çš„è¿é€šåˆ†é‡èšç±»ç®—æ³•ï¼Œæä¾›é«˜æ€§èƒ½å’Œå†…å­˜å‹å¥½çš„èšç±»å®ç°ã€‚
ç‰¹ç‚¹ï¼š
- æ— é¢å¤–ä¾èµ–ï¼šä»…ä½¿ç”¨æ ‡å‡†åº“ï¼Œæ— éœ€NetworkXç­‰ç¬¬ä¸‰æ–¹å›¾åº“
- å¹¶è¡Œä¼˜åŒ–ï¼šå¤šè¿›ç¨‹å¹¶è¡Œè®¡ç®—è¿é€šåˆ†é‡å’Œç°‡éªŒè¯
- å†…å­˜å‹å¥½ï¼šæ™ºèƒ½çš„æ•°æ®åˆ†å—å’Œå†…å­˜ç®¡ç†
- è‡ªé€‚åº”ï¼šæ ¹æ®æ•°æ®è§„æ¨¡è‡ªåŠ¨é€‰æ‹©ä¸²è¡Œæˆ–å¹¶è¡Œç®—æ³•

æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ï¼š
1. æ•°æ®åˆ†å—ï¼šå°†å¤§å›¾åˆ†å—å¹¶è¡Œå¤„ç†
2. æ™ºèƒ½åˆ‡æ¢ï¼šå°æ•°æ®é›†ä½¿ç”¨ä¸²è¡Œé¿å…å¹¶è¡Œå¼€é”€
3. å†…å­˜ç®¡ç†ï¼šé¿å…å¤§å¯¹è±¡çš„è¿›ç¨‹é—´ä¼ é€’
4. è´Ÿè½½å‡è¡¡ï¼šåŠ¨æ€è°ƒæ•´å·¥ä½œè´Ÿè½½åˆ†é…

é€‚ç”¨åœºæ™¯ï¼š
- ä¸­ç­‰è§„æ¨¡æ•°æ®é›†ï¼ˆ1ä¸‡-100ä¸‡èŠ‚ç‚¹ï¼‰
- å¤šæ ¸CPUç¯å¢ƒ
- å†…å­˜å—é™çš„ç”Ÿäº§ç¯å¢ƒ
"""

from __future__ import annotations

import argparse
import multiprocessing as mp
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from functools import partial
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd

from ..utils.config import ensure_output_dir, load_config
from ..utils.io_utils import write_parquet
from ..utils.metrics import StatsRecorder


def build_graph(pairs_df: pd.DataFrame, threshold: float) -> Dict[int, List[Tuple[int, float]]]:
    """
    æ„å»ºåŸºäºCEåˆ†æ•°çš„æ— å‘å›¾ï¼ˆå¹¶è¡Œç‰ˆæœ¬ï¼‰
    
    Args:
        pairs_df: åŒ…å«èŠ‚ç‚¹å¯¹å’ŒCEåˆ†æ•°çš„DataFrame
        threshold: CEåˆ†æ•°é˜ˆå€¼ï¼Œè¿‡æ»¤ä½è´¨é‡è¿æ¥
        
    Returns:
        Dict[int, List[Tuple[int, float]]]: é‚»æ¥è¡¨è¡¨ç¤ºçš„æ— å‘å›¾
        
    æ³¨æ„ï¼š
        æ­¤å‡½æ•°ä¸åŸå§‹ç‰ˆæœ¬ç›¸åŒï¼Œä½†åœ¨å¹¶è¡Œç¯å¢ƒä¸­è°ƒç”¨ã€‚
        æœªæ¥å¯è€ƒè™‘å¯¹è¶…å¤§æ•°æ®é›†è¿›è¡Œåˆ†å—å¹¶è¡Œæ„å»ºã€‚
    """
    g: Dict[int, List[Tuple[int, float]]] = defaultdict(list)
    for i, j, s in zip(pairs_df['i'], pairs_df['j'], pairs_df['ce_final']):
        if s >= threshold:
            g[int(i)].append((int(j), float(s)))
            g[int(j)].append((int(i), float(s)))
    return g


def connected_components_parallel(g: Dict[int, List[Tuple[int, float]]], n_jobs: int = None) -> List[List[int]]:
    """
    å¹¶è¡Œè¿é€šåˆ†é‡è®¡ç®—ä¸»å‡½æ•°
    
    Args:
        g: é‚»æ¥è¡¨è¡¨ç¤ºçš„æ— å‘å›¾
        n_jobs: å¹¶è¡Œè¿›ç¨‹æ•°ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©
        
    Returns:
        List[List[int]]: è¿é€šåˆ†é‡åˆ—è¡¨
        
    ç®—æ³•ç­–ç•¥ï¼š
        1. å°å›¾ï¼ˆâ‰¤1000èŠ‚ç‚¹ï¼‰ï¼šç›´æ¥ä½¿ç”¨ä¸²è¡Œç®—æ³•ï¼Œé¿å…å¹¶è¡Œå¼€é”€
        2. å¤§å›¾ï¼šèŠ‚ç‚¹åˆ†å—ï¼Œæ¯ä¸ªè¿›ç¨‹å¤„ç†ä¸€éƒ¨åˆ†èµ·å§‹èŠ‚ç‚¹
        3. åˆå¹¶é˜¶æ®µï¼šä½¿ç”¨å¹¶æŸ¥é›†åˆå¹¶é‡å çš„è¿é€šåˆ†é‡
        
    æ€§èƒ½ç‰¹ç‚¹ï¼š
        - æ—¶é—´å¤æ‚åº¦: O((V+E)/P)ï¼Œå…¶ä¸­Pæ˜¯è¿›ç¨‹æ•°
        - ç©ºé—´å¤æ‚åº¦: O(V)ï¼Œä¸»è¦ä¸ºå›¾å­˜å‚¨
        - é€‚åˆCPUå¯†é›†å‹åœºæ™¯ï¼ŒIOå¼€é”€è¾ƒå°
        
    æ³¨æ„äº‹é¡¹ï¼š
        - è¿›ç¨‹é—´é€šä¿¡æœ‰å¼€é”€ï¼Œå°å›¾å»ºè®®ä½¿ç”¨ä¸²è¡Œ
        - å›¾å¯¹è±¡ä¼šè¢«å¤åˆ¶åˆ°å„è¿›ç¨‹ï¼Œéœ€æ³¨æ„å†…å­˜ä½¿ç”¨
    """
    if n_jobs is None:
        n_jobs = min(mp.cpu_count(), 8)
    
    nodes = list(g.keys())
    if len(nodes) <= 1000:  # å°å›¾ç›´æ¥ä¸²è¡Œ
        return connected_components_serial(g)
    
    # å°†èŠ‚ç‚¹åˆ†å—ï¼Œæ¯ä¸ªè¿›ç¨‹å¤„ç†ä¸€éƒ¨åˆ†èµ·å§‹èŠ‚ç‚¹
    chunk_size = max(1, len(nodes) // n_jobs)
    node_chunks = [nodes[i:i + chunk_size] for i in range(0, len(nodes), chunk_size)]
    
    # å¹¶è¡ŒæŸ¥æ‰¾è¿é€šåˆ†é‡
    with ProcessPoolExecutor(max_workers=n_jobs) as executor:
        futures = []
        for chunk in node_chunks:
            future = executor.submit(_find_components_chunk, g, chunk)
            futures.append(future)
        
        all_components = []
        for future in as_completed(futures):
            components = future.result()
            all_components.extend(components)
    
    # åˆå¹¶é‡å çš„è¿é€šåˆ†é‡
    return _merge_overlapping_components(all_components)


def connected_components_serial(g: Dict[int, List[Tuple[int, float]]]) -> List[List[int]]:
    """ä¸²è¡Œè¿é€šåˆ†é‡è®¡ç®—ï¼ˆåŸç‰ˆæœ¬ï¼‰"""
    vis = set()
    comps: List[List[int]] = []
    for u in g.keys():
        if u in vis:
            continue
        cur = [u]
        vis.add(u)
        q = [u]
        while q:
            x = q.pop()
            for v, _ in g.get(x, []):
                if v not in vis:
                    vis.add(v)
                    q.append(v)
                    cur.append(v)
        comps.append(sorted(cur))
    return comps


def _find_components_chunk(g: Dict[int, List[Tuple[int, float]]], start_nodes: List[int]) -> List[List[int]]:
    """ä¸ºä¸€ä¸ªèŠ‚ç‚¹å—æŸ¥æ‰¾è¿é€šåˆ†é‡"""
    vis = set()
    components = []
    
    for start_node in start_nodes:
        if start_node in vis or start_node not in g:
            continue
            
        # BFSæŸ¥æ‰¾è¿é€šåˆ†é‡
        component = [start_node]
        vis.add(start_node)
        queue = [start_node]
        
        while queue:
            node = queue.pop(0)
            for neighbor, _ in g.get(node, []):
                if neighbor not in vis:
                    vis.add(neighbor)
                    queue.append(neighbor)
                    component.append(neighbor)
        
        if len(component) > 1:  # åªä¿ç•™æœ‰æ„ä¹‰çš„è¿é€šåˆ†é‡
            components.append(sorted(component))
    
    return components


def _merge_overlapping_components(components: List[List[int]]) -> List[List[int]]:
    """åˆå¹¶é‡å çš„è¿é€šåˆ†é‡

    ä¿®å¤ç‚¹ï¼šåŸå®ç°å¯èƒ½å› â€œé‡åˆ°ç¬¬ä¸€ä¸ªé‡å å°± breakâ€è€Œé—æ¼åç»­åˆå¹¶ï¼Œå¯¼è‡´ç»“æœä¸ç¨³å®šã€‚
    é‡‡ç”¨å¹¶æŸ¥é›†ï¼ˆUnion-Findï¼‰åŸºäºå…ƒç´ åˆå¹¶ï¼Œç¡®ä¿æ‰€æœ‰é‡å åˆ†é‡æœ€ç»ˆåˆå¹¶åˆ°åŒä¸€é›†åˆã€‚
    """
    if not components:
        return []

    parent: Dict[int, int] = {}

    def find(x: int) -> int:
        parent.setdefault(x, x)
        if parent[x] != x:
            parent[x] = find(parent[x])
        return parent[x]

    def union(x: int, y: int) -> None:
        rx, ry = find(x), find(y)
        if rx != ry:
            parent[ry] = rx

    # å°†åŒä¸€ç»„ä»¶å†…çš„æ‰€æœ‰èŠ‚ç‚¹ä¸¤ä¸¤ union
    for comp in components:
        if not comp:
            continue
        base = comp[0]
        for node in comp[1:]:
            union(base, node)

    # èšåˆåˆ°æ ¹ä»£è¡¨
    groups: Dict[int, set] = defaultdict(set)
    for comp in components:
        for node in comp:
            groups[find(node)].add(node)

    # å½¢æˆç»“æœï¼ˆå»é™¤å•èŠ‚ç‚¹ï¼‰
    result: List[List[int]] = []
    for nodes in groups.values():
        if len(nodes) > 1:
            result.append(sorted(nodes))

    return result


def choose_center(nodes: List[int], g: Dict[int, List[Tuple[int, float]]]) -> int:
    """é€‰æ‹©ä¸­å¿ƒç‚¹ - å·²ä¼˜åŒ–"""
    if len(nodes) == 1:
        return nodes[0]
    
    best, best_score = nodes[0], -1.0
    nodes_set = set(nodes)  # ä¼˜åŒ–æŸ¥æ‰¾
    
    for u in nodes:
        neighbors = g.get(u, [])
        ws = [w for v, w in neighbors if v in nodes_set and v != u]
        m = float(np.mean(ws)) if ws else 0.0
        if m > best_score:
            best_score = m
            best = u
    return best


def center_metrics(center: int, nodes: List[int], g: Dict[int, List[Tuple[int, float]]]) -> Dict[str, float]:
    """è®¡ç®—ä¸­å¿ƒç‚¹æŒ‡æ ‡"""
    nodes_set = set(nodes)
    ws = [w for v, w in g.get(center, []) if v in nodes_set and v != center]
    if not ws:
        return {'coverage': 0.0, 'mean': 0.0, 'median': 0.0, 'p10': 0.0}
    
    arr = np.asarray(ws, dtype=float)
    coverage = float(len(ws) / max(1, len(nodes) - 1))
    return {
        'coverage': coverage,
        'mean': float(np.mean(arr)),
        'median': float(np.median(arr)),
        'p10': float(np.percentile(arr, 10)),
    }


def validate_cluster(nodes: List[int], g: Dict[int, List[Tuple[int, float]]], cons: Dict[str, float]) -> Tuple[bool, int, Dict[str, float]]:
    """éªŒè¯ç°‡è´¨é‡"""
    center = choose_center(nodes, g)
    metrics = center_metrics(center, nodes, g)
    ok = (
        metrics['coverage'] >= cons.get('coverage', 0.85)
        and metrics['mean'] >= cons.get('mean', 0.85)
        and metrics['median'] >= cons.get('median', 0.845)
        and metrics['p10'] >= cons.get('p10', 0.80)
    )
    return ok, center, metrics


def validate_clusters_parallel(components: List[List[int]], g: Dict[int, List[Tuple[int, float]]], 
                             cons: Dict[str, float], min_cluster_size: int, 
                             n_jobs: int = None) -> List[Dict]:
    """å¹¶è¡ŒéªŒè¯ç°‡"""
    if n_jobs is None:
        n_jobs = min(mp.cpu_count(), len(components))
    
    if len(components) <= 10:  # å°æ•°æ®é›†ç›´æ¥ä¸²è¡Œ
        return validate_clusters_serial(components, g, cons, min_cluster_size)
    
    # å¹¶è¡ŒéªŒè¯
    validate_func = partial(validate_single_component, g=g, cons=cons, min_cluster_size=min_cluster_size)
    
    with ThreadPoolExecutor(max_workers=n_jobs) as executor:
        futures = [executor.submit(validate_func, comp) for comp in components]
        
        valid_clusters = []
        for future in as_completed(futures):
            result = future.result()
            if result:
                valid_clusters.extend(result)
    
    return valid_clusters


def validate_clusters_serial(components: List[List[int]], g: Dict[int, List[Tuple[int, float]]], 
                           cons: Dict[str, float], min_cluster_size: int) -> List[Dict]:
    """ä¸²è¡ŒéªŒè¯ç°‡ï¼ˆåŸç‰ˆæœ¬é€»è¾‘ï¼‰"""
    valid_clusters = []
    
    for nodes in components:
        if len(nodes) < min_cluster_size:
            continue
            
        ok, center, metrics = validate_cluster(nodes, g, cons)
        if not ok:
            # åŒç‚¹ç°‡ä¿æŠ¤
            pair_min = float(cons.get('pair_min_for_2_nodes', 0.86))
            kept_pairs = []
            for u in nodes:
                for v, w in g.get(u, []):
                    if v > u and v in nodes and w >= pair_min:
                        kept_pairs.append([u, v])
            if not kept_pairs:
                continue
            for comp in kept_pairs:
                ok2, center2, metrics2 = validate_cluster(comp, g, cons)
                if ok2:
                    valid_clusters.append({'center': center2, 'members': comp, **metrics2})
            continue
        valid_clusters.append({'center': center, 'members': nodes, **metrics})
    
    return valid_clusters


def validate_single_component(nodes: List[int], g: Dict[int, List[Tuple[int, float]]], 
                            cons: Dict[str, float], min_cluster_size: int) -> List[Dict]:
    """éªŒè¯å•ä¸ªè¿é€šåˆ†é‡"""
    if len(nodes) < min_cluster_size:
        return []
    
    ok, center, metrics = validate_cluster(nodes, g, cons)
    if not ok:
        # åŒç‚¹ç°‡ä¿æŠ¤
        pair_min = float(cons.get('pair_min_for_2_nodes', 0.86))
        kept_pairs = []
        for u in nodes:
            for v, w in g.get(u, []):
                if v > u and v in nodes and w >= pair_min:
                    kept_pairs.append([u, v])
        if not kept_pairs:
            return []
        
        result = []
        for comp in kept_pairs:
            ok2, center2, metrics2 = validate_cluster(comp, g, cons)
            if ok2:
                result.append({'center': center2, 'members': comp, **metrics2})
        return result
    
    return [{'center': center, 'members': nodes, **metrics}]


def consistency_vote(i: int, j: int, emb_a: np.ndarray, emb_b: np.ndarray, emb_c: np.ndarray, 
                    std_max: float, cos_a: float, cos_b: float, cos_c: float, vote_2_of_3: bool) -> bool:
    """ä¸€è‡´æ€§æŠ•ç¥¨"""
    ca = float(np.dot(emb_a[i], emb_a[j]))
    cb = float(np.dot(emb_b[i], emb_b[j]))
    cc = float(np.dot(emb_c[i], emb_c[j]))
    votes = (1 if ca >= cos_a else 0) + (1 if cb >= cos_b else 0) + (1 if cc >= cos_c else 0)
    std = float(np.std([ca, cb, cc]))
    return ((votes >= 2) if vote_2_of_3 else (votes == 3)) and (std <= std_max)


def run(cfg_path: str, input_file: str = None, n_jobs: Optional[int] = None) -> None:
    """
    å¹¶è¡Œå¼•æ“ä¸»å‡½æ•° - å¤šæ ¸ä¼˜åŒ–çš„è¿é€šåˆ†é‡èšç±»
    
    Args:
        cfg_path: é…ç½®æ–‡ä»¶è·¯å¾„
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆæœªä½¿ç”¨ï¼Œä¿æŒæ¥å£ä¸€è‡´æ€§ï¼‰
        n_jobs: å¹¶è¡Œè¿›ç¨‹æ•°ï¼Œ-1è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
        
    ç®—æ³•æµç¨‹ï¼š
        1. é…ç½®åŠ è½½ï¼šè§£æå¹¶è¡Œå‚æ•°å’Œèšç±»é…ç½®
        2. æ•°æ®åŠ è½½ï¼šä»pair_scores.parquetåŠ è½½CEåˆ†æ•°
        3. å›¾æ„å»ºï¼šåŸºäºé«˜ç½®ä¿¡åº¦é˜ˆå€¼æ„å»ºæ— å‘å›¾
        4. å¹¶è¡Œèšç±»ï¼šå¤šè¿›ç¨‹å¹¶è¡Œè®¡ç®—è¿é€šåˆ†é‡
        5. å¹¶è¡ŒéªŒè¯ï¼šå¤šçº¿ç¨‹å¹¶è¡ŒéªŒè¯ç°‡è´¨é‡
        6. äºŒæ¬¡èšåˆï¼šä¸²è¡Œæ‰§è¡Œç°‡é—´åˆå¹¶
        7. ç»“æœè¾“å‡ºï¼šç”Ÿæˆèšç±»ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
        
    æ€§èƒ½ä¼˜åŒ–ï¼š
        - è‡ªé€‚åº”å¹¶è¡Œï¼šæ ¹æ®æ•°æ®è§„æ¨¡è‡ªåŠ¨é€‰æ‹©ä¸²è¡Œ/å¹¶è¡Œ
        - å†…å­˜ä¼˜åŒ–ï¼šé¿å…ä¸å¿…è¦çš„æ•°æ®å¤åˆ¶
        - è´Ÿè½½å‡è¡¡ï¼šåŠ¨æ€è°ƒæ•´å·¥ä½œè´Ÿè½½
        - æ™ºèƒ½åˆ‡æ¢ï¼šå°æ•°æ®é›†é¿å…å¹¶è¡Œå¼€é”€
        
    æ€§èƒ½é¢„æœŸï¼š
        - ä¸­ç­‰æ•°æ®é›†ï¼ˆ1ä¸‡-10ä¸‡èŠ‚ç‚¹ï¼‰ï¼š2-4xåŠ é€Ÿ
        - å¤§æ•°æ®é›†ï¼ˆ>10ä¸‡èŠ‚ç‚¹ï¼‰ï¼š4-8xåŠ é€Ÿ
        - å°æ•°æ®é›†ï¼ˆ<1000èŠ‚ç‚¹ï¼‰ï¼šä¸ä¸²è¡Œæ€§èƒ½ç›¸å½“
        
    è¾“å‡ºæ–‡ä»¶ï¼š
        - clusters.parquet: èšç±»ç»“æœ
        - stage_stats.json: åŒ…å«å¹¶è¡Œæ€§èƒ½ç»Ÿè®¡
        - å¯é€‰å›¾è¡¨: cluster_size_hist.png
    """
    cfg = load_config(cfg_path)
    out_dir = ensure_output_dir(cfg)
    stats = StatsRecorder(cfg.get('observe.stats_path', f"{out_dir}/stage_stats.json"))
    
    # è·å–å¹¶è¡Œé…ç½®ï¼ˆæ”¯æŒ -1 è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒï¼‰
    if n_jobs is None:
        cfg_n_jobs = cfg.get('cluster.n_jobs', -1)
        if cfg_n_jobs in (-1, 0, None):
            n_jobs = mp.cpu_count()
        else:
            try:
                n_jobs = int(cfg_n_jobs)
            except Exception:
                n_jobs = mp.cpu_count()
    n_jobs = max(1, min(n_jobs, mp.cpu_count()))
    
    print(f"[stage4] ä½¿ç”¨ {n_jobs} ä¸ªCPUæ ¸å¿ƒè¿›è¡Œå¹¶è¡Œèšç±»")
    
    pairs = pd.read_parquet(f"{out_dir}/stage3_ranked_pairs.parquet")
    print(f"[stage4] åŠ è½½ {len(pairs)} ä¸ªç›¸ä¼¼å¯¹")

    # æ„æ ¸å¿ƒå›¾ï¼ˆé«˜ç½®ä¿¡ï¼‰
    high_th = float(cfg.get('rerank.thresholds.high', 0.83))
    g = build_graph(pairs, high_th)
    print(f"[stage4] æ„å»ºå›¾å®Œæˆï¼ŒèŠ‚ç‚¹æ•°: {len(g)}")

    # å¹¶è¡Œèšç±»
    use_parallel = cfg.get('cluster.use_parallel', True) and len(g) > 100
    if use_parallel:
        print("[stage4] ä½¿ç”¨å¹¶è¡Œè¿é€šåˆ†é‡ç®—æ³•")
        comps = connected_components_parallel(g, n_jobs)
        method_used = 'connected_components_parallel'
    else:
        print("[stage4] ä½¿ç”¨ä¸²è¡Œè¿é€šåˆ†é‡ç®—æ³•")
        comps = connected_components_serial(g)
        method_used = 'connected_components_serial'
    
    print(f"[stage4] å‘ç° {len(comps)} ä¸ªè¿é€šåˆ†é‡")

    # å¹¶è¡Œç°‡éªŒè¯
    cons = cfg.get('cluster.center_constraints', {})
    min_cluster_size = int(cfg.get('cluster.min_cluster_size', 2))
    
    if use_parallel and len(comps) > 10:
        print("[stage4] ä½¿ç”¨å¹¶è¡Œç°‡éªŒè¯")
        valid_clusters = validate_clusters_parallel(comps, g, cons, min_cluster_size, n_jobs)
    else:
        print("[stage4] ä½¿ç”¨ä¸²è¡Œç°‡éªŒè¯")
        valid_clusters = validate_clusters_serial(comps, g, cons, min_cluster_size)
    
    print(f"[stage4] éªŒè¯åä¿ç•™ {len(valid_clusters)} ä¸ªæœ‰æ•ˆç°‡")

    # äºŒæ¬¡èšåˆï¼ˆè¿™éƒ¨åˆ†è¾ƒéš¾å¹¶è¡ŒåŒ–ï¼Œä¿æŒä¸²è¡Œï¼‰
    second_cfg = cfg.get('cluster.second_merge', {})
    if second_cfg.get('enable', True) and len(valid_clusters) >= 2:
        print("[stage4] å¼€å§‹äºŒæ¬¡èšåˆ...")
        ce_min = float(second_cfg.get('ce_min', 0.81))
        require_vote = bool(second_cfg.get('require_consistency_vote', True))
        
        # æ„å»ºè¾¹æ˜ å°„
        edge_map: Dict[Tuple[int, int], float] = {}
        for i, j, s in zip(pairs['i'], pairs['j'], pairs['ce_final']):
            a, b = (int(i), int(j)) if i < j else (int(j), int(i))
            if s >= ce_min:
                edge_map[(a, b)] = float(s)
        
        # åŠ è½½åµŒå…¥
        emb_a = np.load(f"{out_dir}/emb_a.npy")
        emb_b = np.load(f"{out_dir}/emb_b.npy")
        emb_c = np.load(f"{out_dir}/emb_c.npy")
        
        cons_cfg = cfg.get('consistency', {})
        cos_a_th = float(cons_cfg.get('cos_a', 0.875))
        cos_b_th = float(cons_cfg.get('cos_b', 0.870))
        cos_c_th = float(cons_cfg.get('cos_c', 0.870))
        std_max = float(cons_cfg.get('std_max', 0.04))
        vote_2 = bool(cons_cfg.get('vote_2_of_3', True))

        merged = True
        merge_rounds = 0
        while merged:
            merged = False
            merge_rounds += 1
            K = len(valid_clusters)
            if K <= 1:
                break
            
            print(f"[stage4] äºŒæ¬¡èšåˆç¬¬ {merge_rounds} è½®ï¼Œå½“å‰ç°‡æ•°: {K}")
            
            done = False
            for x in range(K):
                for y in range(x + 1, K):
                    cx = valid_clusters[x]['center']
                    cy = valid_clusters[y]['center']
                    a, b = (min(cx, cy), max(cx, cy))
                    ce = edge_map.get((a, b), 0.0)
                    if ce < ce_min:
                        continue
                    if require_vote and not consistency_vote(cx, cy, emb_a, emb_b, emb_c, std_max, cos_a_th, cos_b_th, cos_c_th, vote_2):
                        continue
                    new_nodes = sorted(set(valid_clusters[x]['members']) | set(valid_clusters[y]['members']))
                    ok, center_new, metrics_new = validate_cluster(new_nodes, g, cons)
                    if not ok:
                        continue
                    new_entry = {'center': center_new, 'members': new_nodes, **metrics_new}
                    keep = [valid_clusters[k] for k in range(K) if k not in (x, y)]
                    keep.append(new_entry)
                    valid_clusters = keep
                    merged = True
                    done = True
                    break
                if done:
                    break
        
        print(f"[stage4] äºŒæ¬¡èšåˆå®Œæˆï¼Œæœ€ç»ˆç°‡æ•°: {len(valid_clusters)}")

    # è¾“å‡º
    clusters_rows = []
    for cid, c in enumerate(valid_clusters):
        clusters_rows.append({
            'cluster_id': cid,
            'center': int(c['center']),
            'members': list(map(int, c['members'])),
            'coverage': float(c['coverage']),
            'mean': float(c['mean']),
            'median': float(c['median']),
            'p10': float(c['p10']),
        })
    clusters_df = pd.DataFrame(clusters_rows)
    write_parquet(clusters_df, f"{out_dir}/clusters.parquet")

    # ç»Ÿè®¡
    stats_dict = {
        'num_clusters': int(len(valid_clusters)),
        'cluster_method': method_used,
        'n_jobs_used': int(n_jobs),
    }
    sizes = [len(m['members']) for m in valid_clusters]
    if sizes:
        stats_dict.update({
            'size_p50': float(np.median(sizes)),
            'size_p90': float(np.percentile(sizes, 90)),
            'size_max': int(max(sizes)),
        })
        if cfg.get('observe.save_histograms', True):
            stats.histogram_png(sizes, f"{out_dir}/figs/cluster_size_hist.png", title='Cluster size distribution')
    else:
        stats_dict.update({'size_p50': 0.0, 'size_p90': 0.0, 'size_max': 0})

    stats.update('stage4', stats_dict)
    print(f"[stage4] èšç±»å®Œæˆï¼Œæœ€ç»ˆè¾“å‡º {len(valid_clusters)} ä¸ªç°‡")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Stage4 èšç±»æ¨¡å— - å¹¶è¡Œå¼•æ“ï¼ˆé«˜æ€§èƒ½ç‰ˆæœ¬ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
å¼•æ“ç‰¹ç‚¹ï¼š
  ğŸš€ é«˜æ€§èƒ½ï¼šå¤šæ ¸å¹¶è¡Œä¼˜åŒ–ï¼Œ2-8xæ€§èƒ½æå‡
  ğŸ’¾ å†…å­˜å‹å¥½ï¼šæ™ºèƒ½æ•°æ®åˆ†å—å’Œå†…å­˜ç®¡ç†
  ğŸ”§ æ— ä¾èµ–ï¼šä»…ä½¿ç”¨æ ‡å‡†åº“ï¼Œæ— éœ€ç¬¬ä¸‰æ–¹å›¾åº“
  ğŸ§  æ™ºèƒ½åŒ–ï¼šè‡ªé€‚åº”é€‰æ‹©ä¸²è¡Œ/å¹¶è¡Œç®—æ³•
  
æ€§èƒ½æ•°æ®ï¼š
  å°æ•°æ®é›†  (<1KèŠ‚ç‚¹)   : ä¸²è¡Œæ¨¡å¼ï¼Œé¿å…å¼€é”€
  ä¸­ç­‰æ•°æ®é›†(1K-100KèŠ‚ç‚¹): 2-4xåŠ é€Ÿ
  å¤§æ•°æ®é›†  (>100KèŠ‚ç‚¹)  : 4-8xåŠ é€Ÿ
  
ç®—æ³•ï¼š
  - å¤šè¿›ç¨‹å¹¶è¡Œè¿é€šåˆ†é‡æ£€æµ‹
  - å¹¶æŸ¥é›†ä¼˜åŒ–çš„åˆ†é‡åˆå¹¶
  - å¤šçº¿ç¨‹å¹¶è¡Œç°‡éªŒè¯
  - äºŒæ¬¡èšåˆä¼˜åŒ–
        """
    )
    
    parser.add_argument(
        '--config', '-c',
        default='src/configs/config.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: %(default)s)'
    )
    parser.add_argument(
        '--input', '-i',
        help='è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ï¼‰'
    )
    parser.add_argument(
        '--n-jobs', '-j',
        type=int,
        help='å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆ-1è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒï¼Œ0è¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©ï¼‰'
    )
    
    args = parser.parse_args()
    run(args.config, args.input, args.n_jobs)

