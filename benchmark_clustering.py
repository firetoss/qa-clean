#!/usr/bin/env python3
"""
èšç±»æ€§èƒ½å¯¹æ¯”è„šæœ¬
æ¯”è¾ƒä¸²è¡Œç‰ˆæœ¬å’Œå¹¶è¡Œç‰ˆæœ¬çš„æ€§èƒ½å·®å¼‚
"""

import time
import multiprocessing as mp
import pandas as pd
import numpy as np
from src.stages.stage4_cluster_original import run as run_original
from src.stages.stage4_cluster_parallel import run as run_parallel
from src.stages.stage4_cluster_networkx import run as run_networkx
from src.stages.stage4_cluster import run as run_unified

def create_test_data(n_pairs: int = 10000, output_dir: str = "./outputs") -> None:
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    import os
    os.makedirs(output_dir, exist_ok=True)
    
    # ç”Ÿæˆéšæœºç›¸ä¼¼å¯¹æ•°æ®
    np.random.seed(42)
    pairs_data = {
        'i': np.random.randint(0, n_pairs//10, n_pairs),
        'j': np.random.randint(0, n_pairs//10, n_pairs),
        'ce_final': np.random.beta(2, 5, n_pairs)  # åå‘è¾ƒä½åˆ†æ•°çš„åˆ†å¸ƒ
    }
    
    # ç¡®ä¿ i != j
    mask = pairs_data['i'] != pairs_data['j']
    pairs_df = pd.DataFrame({
        'i': pairs_data['i'][mask],
        'j': pairs_data['j'][mask], 
        'ce_final': pairs_data['ce_final'][mask]
    })
    
    pairs_df.to_parquet(f"{output_dir}/pair_scores.parquet")
    
    # åˆ›å»ºå‡çš„åµŒå…¥æ•°æ®
    n_nodes = max(pairs_df['i'].max(), pairs_df['j'].max()) + 1
    emb_dim = 768
    
    for name in ['emb_a', 'emb_b', 'emb_c']:
        emb = np.random.randn(n_nodes, emb_dim).astype(np.float32)
        # æ ‡å‡†åŒ–
        emb = emb / np.linalg.norm(emb, axis=1, keepdims=True)
        np.save(f"{output_dir}/{name}.npy", emb)
    
    print(f"åˆ›å»ºæµ‹è¯•æ•°æ®å®Œæˆï¼š{len(pairs_df)} ä¸ªç›¸ä¼¼å¯¹ï¼Œ{n_nodes} ä¸ªèŠ‚ç‚¹")

def benchmark_clustering():
    """ä¸‰ç§èšç±»å¼•æ“æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    print("ğŸš€ ä¸‰ç§èšç±»å¼•æ“æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    create_test_data(n_pairs=50000)  # 5ä¸‡ä¸ªç›¸ä¼¼å¯¹
    
    config_path = "src/configs/config.yaml"
    
    print(f"CPUæ ¸å¿ƒæ•°: {mp.cpu_count()}")
    print()
    
    results = {}
    
    # æµ‹è¯•åŸå§‹ç‰ˆæœ¬
    print("ğŸ“Š æµ‹è¯•Originalå¼•æ“ï¼ˆåŸå§‹å•æ ¸ï¼‰...")
    start_time = time.time()
    try:
        run_original(config_path)
        original_time = time.time() - start_time
        results['original'] = original_time
        print(f"âœ… Originalå¼•æ“å®Œæˆï¼Œè€—æ—¶: {original_time:.2f}ç§’")
    except Exception as e:
        print(f"âŒ Originalå¼•æ“å¤±è´¥: {e}")
        results['original'] = float('inf')
    
    print()
    
    # æµ‹è¯•å¹¶è¡Œç‰ˆæœ¬
    print("âš¡ æµ‹è¯•Parallelå¼•æ“ï¼ˆå¤šæ ¸å¹¶è¡Œï¼‰...")
    start_time = time.time()
    try:
        run_parallel(config_path)
        parallel_time = time.time() - start_time
        results['parallel'] = parallel_time
        print(f"âœ… Parallelå¼•æ“å®Œæˆï¼Œè€—æ—¶: {parallel_time:.2f}ç§’")
    except Exception as e:
        print(f"âŒ Parallelå¼•æ“å¤±è´¥: {e}")
        results['parallel'] = float('inf')
    
    print()
    
    # æµ‹è¯•NetworkXç‰ˆæœ¬
    print("ğŸŒ æµ‹è¯•NetworkXå¼•æ“ï¼ˆé«˜çº§ç®—æ³•ï¼‰...")
    start_time = time.time()
    try:
        run_networkx(config_path)
        networkx_time = time.time() - start_time
        results['networkx'] = networkx_time
        print(f"âœ… NetworkXå¼•æ“å®Œæˆï¼Œè€—æ—¶: {networkx_time:.2f}ç§’")
    except Exception as e:
        print(f"âŒ NetworkXå¼•æ“å¤±è´¥: {e}")
        results['networkx'] = float('inf')
    
    print()
    
    # æ€§èƒ½å¯¹æ¯”åˆ†æ
    print("ğŸ“ˆ æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    print("=" * 60)
    
    valid_results = {k: v for k, v in results.items() if v != float('inf')}
    
    if len(valid_results) >= 2:
        baseline = min(valid_results.values())
        
        print(f"{'å¼•æ“':<12} {'è€—æ—¶(ç§’)':<10} {'ç›¸å¯¹æ€§èƒ½':<12} {'æ¨èåœºæ™¯'}")
        print("-" * 60)
        
        for engine, time_val in results.items():
            if time_val == float('inf'):
                relative = "å¤±è´¥"
                scenario = "ä¾èµ–ç¼ºå¤±"
            else:
                speedup = baseline / time_val
                if speedup >= 1.0:
                    relative = f"{speedup:.2f}x"
                else:
                    relative = f"1/{1/speedup:.2f}x"
                
                if engine == 'original':
                    scenario = "å°æ•°æ®é›†ï¼Œæœ€å°ä¾èµ–"
                elif engine == 'parallel':
                    scenario = "ä¸­å¤§æ•°æ®é›†ï¼Œæ€§èƒ½ä¼˜å…ˆ"
                else:  # networkx
                    scenario = "è´¨é‡ä¼˜å…ˆï¼Œç”Ÿäº§ç¯å¢ƒ"
            
            print(f"{engine:<12} {time_val if time_val != float('inf') else 'N/A':<10} {relative:<12} {scenario}")
        
        print()
        
        # æ‰¾å‡ºæœ€ä½³å¼•æ“
        best_engine = min(valid_results.keys(), key=lambda k: valid_results[k])
        best_time = valid_results[best_engine]
        
        print(f"ğŸ† æœ€ä½³æ€§èƒ½: {best_engine} å¼•æ“ ({best_time:.2f}ç§’)")
        
        if len(valid_results) >= 2:
            sorted_results = sorted(valid_results.items(), key=lambda x: x[1])
            if len(sorted_results) >= 2:
                second_best = sorted_results[1]
                speedup = second_best[1] / best_time
                print(f"ğŸ“Š ç›¸æ¯”ç¬¬äºŒåæå‡: {speedup:.2f}x")
    
    print()
    print("ğŸ’¡ é€‰æ‹©å»ºè®®:")
    print("- å°æ•°æ®é›†(<1000èŠ‚ç‚¹): originalå¼•æ“ï¼Œç®€å•å¯é ")
    print("- ä¸­ç­‰æ•°æ®é›†(1000-10000èŠ‚ç‚¹): parallelå¼•æ“ï¼Œæ€§èƒ½ä¼˜ç§€")  
    print("- å¤§æ•°æ®é›†(>10000èŠ‚ç‚¹): networkxå¼•æ“ï¼Œè´¨é‡æœ€ä½³")
    print("- ç”Ÿäº§ç¯å¢ƒ: networkxå¼•æ“ï¼ŒåŠŸèƒ½ä¸°å¯Œ")
    print()
    print("ğŸ”§ é…ç½®æ–¹æ³•:")
    print("åœ¨ src/configs/config.yaml ä¸­è®¾ç½®:")
    print('cluster:')
    print('  engine: "networkx"  # æˆ– "parallel" æˆ– "original"')

if __name__ == "__main__":
    benchmark_clustering()
