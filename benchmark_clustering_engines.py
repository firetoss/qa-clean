#!/usr/bin/env python3
"""
ä¸‰ç§èšç±»å¼•æ“æ€§èƒ½å¯¹æ¯”è„šæœ¬
æµ‹è¯• original, parallel, networkx ä¸‰ç§å¼•æ“çš„æ€§èƒ½å·®å¼‚
"""

import time
import multiprocessing as mp
import pandas as pd
import numpy as np
import tempfile
import shutil
from pathlib import Path


def create_test_data(n_pairs: int = 10000, output_dir: str = "./outputs") -> None:
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    import os
    os.makedirs(output_dir, exist_ok=True)
    
    # ç”Ÿæˆéšæœºç›¸ä¼¼å¯¹æ•°æ®
    np.random.seed(42)
    pairs_data = {
        'i': np.random.randint(0, n_pairs//10, n_pairs),
        'j': np.random.randint(0, n_pairs//10, n_pairs),
        'ce_final': np.random.beta(2, 5, n_pairs)  # åå‘è¾ƒä½åˆ†æ•°çš„åˆ†å¸ƒ
    }
    
    # ç¡®ä¿ i != j
    mask = pairs_data['i'] != pairs_data['j']
    pairs_df = pd.DataFrame({
        'i': pairs_data['i'][mask],
        'j': pairs_data['j'][mask], 
        'ce_final': pairs_data['ce_final'][mask]
    })
    
    pairs_df.to_parquet(f"{output_dir}/pair_scores.parquet")
    
    # åˆ›å»ºå‡çš„åµŒå…¥æ•°æ®
    n_nodes = max(pairs_df['i'].max(), pairs_df['j'].max()) + 1
    emb_dim = 768
    
    for name in ['emb_a', 'emb_b', 'emb_c']:
        emb = np.random.randn(n_nodes, emb_dim).astype(np.float32)
        # æ ‡å‡†åŒ–
        emb = emb / np.linalg.norm(emb, axis=1, keepdims=True)
        np.save(f"{output_dir}/{name}.npy", emb)
    
    print(f"åˆ›å»ºæµ‹è¯•æ•°æ®å®Œæˆï¼š{len(pairs_df)} ä¸ªç›¸ä¼¼å¯¹ï¼Œ{n_nodes} ä¸ªèŠ‚ç‚¹")


def create_test_config(engine: str, output_dir: str) -> str:
    """åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶"""
    config_content = f"""# æµ‹è¯•é…ç½®æ–‡ä»¶
pipeline:
  language: "zh"
  random_seed: 42
  output_dir: "{output_dir}"

data:
  input_path: "./data/raw/input.parquet"
  q_col: "question"
  a_col: "answer"

embeddings:
  models:
    a: "BAAI/bge-large-zh-v1.5"
    b: "moka-ai/m3e-large"
    c: "Alibaba-NLP/gte-large-zh"

consistency:
  cos_a: 0.875
  cos_b: 0.870
  cos_c: 0.870
  std_max: 0.04
  vote_2_of_3: true

rerank:
  thresholds:
    high: 0.83
    mid_low: 0.77

cluster:
  engine: "{engine}"
  method: "leiden"
  min_cluster_size: 2
  use_parallel: true
  n_jobs: -1
  resolution: 1.0
  
  center_constraints:
    coverage: 0.85
    mean: 0.85
    median: 0.845
    p10: 0.80
    pair_min_for_2_nodes: 0.86
  
  second_merge:
    enable: true
    ce_min: 0.81
    require_consistency_vote: true

observe:
  save_histograms: false
"""
    
    config_path = f"test_config_{engine}.yaml"
    with open(config_path, 'w', encoding='utf-8') as f:
        f.write(config_content)
    return config_path


def test_engine(engine: str, config_path: str) -> dict:
    """æµ‹è¯•å•ä¸ªå¼•æ“"""
    print(f"\nğŸ§ª æµ‹è¯• {engine.upper()} å¼•æ“...")
    
    start_time = time.time()
    success = True
    error_msg = ""
    
    try:
        from src.stages.stage4_cluster import run
        run(config_path)
        elapsed_time = time.time() - start_time
        print(f"âœ… {engine.upper()} å¼•æ“å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
    except Exception as e:
        elapsed_time = float('inf')
        success = False
        error_msg = str(e)
        print(f"âŒ {engine.upper()} å¼•æ“å¤±è´¥: {e}")
    
    return {
        'engine': engine,
        'success': success,
        'time': elapsed_time,
        'error': error_msg
    }


def benchmark_all_engines():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•æ‰€æœ‰å¼•æ“"""
    print("ğŸš€ ä¸‰ç§èšç±»å¼•æ“æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        print(f"ä½¿ç”¨ä¸´æ—¶ç›®å½•: {temp_dir}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        create_test_data(n_pairs=20000, output_dir=temp_dir)  # 2ä¸‡ä¸ªç›¸ä¼¼å¯¹
        
        print(f"CPUæ ¸å¿ƒæ•°: {mp.cpu_count()}")
        
        engines = ['original', 'parallel', 'networkx']
        results = []
        
        # æµ‹è¯•æ¯ä¸ªå¼•æ“
        for engine in engines:
            # åˆ›å»ºå¼•æ“ä¸“ç”¨çš„é…ç½®æ–‡ä»¶
            config_path = create_test_config(engine, temp_dir)
            
            try:
                result = test_engine(engine, config_path)
                results.append(result)
            finally:
                # æ¸…ç†é…ç½®æ–‡ä»¶
                Path(config_path).unlink(missing_ok=True)
            
            # æ¸…ç†è¾“å‡ºæ–‡ä»¶ï¼Œå‡†å¤‡ä¸‹ä¸€ä¸ªæµ‹è¯•
            clusters_file = Path(temp_dir) / "clusters.parquet"
            clusters_file.unlink(missing_ok=True)
    
    # è¾“å‡ºå¯¹æ¯”ç»“æœ
    print("\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    print("=" * 60)
    
    successful_results = [r for r in results if r['success']]
    
    if len(successful_results) >= 2:
        # æŒ‰æ—¶é—´æ’åº
        successful_results.sort(key=lambda x: x['time'])
        
        print("æ’å | å¼•æ“      | è€—æ—¶(ç§’) | ç›¸å¯¹æ€§èƒ½")
        print("-" * 45)
        
        base_time = successful_results[0]['time']
        for i, result in enumerate(successful_results):
            relative_perf = base_time / result['time'] if result['time'] > 0 else 1.0
            print(f"{i+1:2d}   | {result['engine']:8s} | {result['time']:7.2f} | {relative_perf:.2f}x")
        
        # è¯¦ç»†åˆ†æ
        print("\nğŸ” è¯¦ç»†åˆ†æ:")
        print("-" * 30)
        
        if len(successful_results) >= 3:
            fastest = successful_results[0]
            slowest = successful_results[-1]
            speedup = slowest['time'] / fastest['time']
            print(f"æœ€å¿«å¼•æ“: {fastest['engine'].upper()}")
            print(f"æœ€æ…¢å¼•æ“: {slowest['engine'].upper()}")
            print(f"æ€§èƒ½å·®å¼‚: {speedup:.2f}x")
        
        # æ¨è
        print("\nğŸ’¡ æ¨èæ–¹æ¡ˆ:")
        print("-" * 20)
        
        for result in successful_results:
            if result['engine'] == 'networkx':
                print(f"ğŸ¥‡ {result['engine'].upper()}: åŠŸèƒ½æœ€å¼ºï¼Œæ”¯æŒå¤šç§é«˜çº§èšç±»ç®—æ³•")
            elif result['engine'] == 'parallel':
                print(f"âš¡ {result['engine'].upper()}: æ€§èƒ½ä¼˜åŒ–ï¼Œæ— é¢å¤–ä¾èµ–")
            elif result['engine'] == 'original':
                print(f"ğŸ”§ {result['engine'].upper()}: æœ€å°ä¾èµ–ï¼Œè°ƒè¯•å‹å¥½")
    
    # æ˜¾ç¤ºå¤±è´¥çš„å¼•æ“
    failed_results = [r for r in results if not r['success']]
    if failed_results:
        print("\nâŒ å¤±è´¥çš„å¼•æ“:")
        print("-" * 20)
        for result in failed_results:
            print(f"{result['engine'].upper()}: {result['error']}")
            if 'networkx' in result['error'].lower() or 'import' in result['error'].lower():
                print("  ğŸ’¡ æç¤º: å¯èƒ½éœ€è¦å®‰è£… NetworkX ç›¸å…³ä¾èµ–")
                print("     pip install networkx python-igraph leidenalg")
    
    print("\nğŸ¯ é…ç½®å»ºè®®:")
    print("-" * 15)
    print("åœ¨ src/configs/config.yaml ä¸­è®¾ç½®:")
    print("cluster:")
    print("  engine: \"networkx\"    # é»˜è®¤æ¨è")
    print("  # engine: \"parallel\"   # æ€§èƒ½ä¼˜å…ˆï¼Œæ— é¢å¤–ä¾èµ–")
    print("  # engine: \"original\"   # æœ€å°ä¾èµ–")


if __name__ == "__main__":
    benchmark_all_engines()

